# üß† Machine Learning System Design & Reporting Framework (Student Edition)

> \
> \
> **Goal:** This is a structured format you must follow for all ML project writeups.


---

## üóÇÔ∏è 1. Project Overview

**Purpose:** Briefly describe what this ML system does and why it exists.

**Prompts:**

* What is the system you‚Äôre building?
* What real-world problem does it solve?
* Who are the users or stakeholders?
* What‚Äôs the *one-line* goal of the system?


---

## ‚úÖ 2. Clarifying Requirements

**Purpose:** Define the project scope, business motivation, and constraints.

**Prompts:**

* **Business Objective**: What is the goal? (e.g., increase retention, reduce churn)
* **Success Metric**: How will you measure if the goal is achieved?
* **Key Features**: What interactions or data inputs are important? (e.g., user clicks, likes)
* **Constraints**:
  * Are there memory, latency, or deployment limitations?
  * Does it run on cloud or mobile?
* **Scale**:
  * How many users/items/transactions per day?
  * Expected growth?
* **Other considerations**: Are there privacy, fairness, or compliance issues?


---

## üéØ 3. Frame the Problem as an ML Task

**Purpose:** Translate the problem into a concrete ML formulation.

**Prompts:**

* What kind of ML task is this? (classification, regression, ranking, etc.)
* What is your **ML objective**?
  * Be precise. E.g., ‚ÄúPredict whether a user will click on a product.‚Äù
* What is the **input** to your model?
* What is the **output** of your model?
* If applicable: What other ways could this input/output be structured?
* Is ML even needed here, or is a rules-based solution good enough?


---

## ‚öñÔ∏è 4. Assumptions and Trade-offs

**Purpose:** Document all decisions that might affect your design.

**Prompts:**

* What assumptions are you making about the data, users, or system?
* What trade-offs did you make? (e.g., accuracy vs. latency, model size vs. performance)
* What parts did you deliberately **de-scope** or simplify?


---

## üßπ 5. Data Preparation

### a) Data Sources & Storage

* Where is the data coming from?
* Is it raw, cleaned, labeled, or user-generated?
* How is it stored? (SQL, NoSQL, flat files, cloud buckets, etc.)

### b) Data Types

* What types of data are you using?
  * Structured (numeric, categorical)
  * Unstructured (text, images, audio)
* Any multimodal combinations?

### c) Feature Engineering

* What features are you using?
* Why are they relevant?
* Did you create new features? How?
* What transformations were applied?

### d) Handling Missing or Dirty Data

* Did you drop, impute, or ignore missing values?
* If imputing: what method? Why?

### e) Scaling / Normalization

* Did you normalize/standardize features? (How?)
* Did you use log scaling or bucketing?

### f) Encoding Categorical Features

* Which encoding method(s) did you use?
  * Integer, one-hot, embeddings


---

## üõ†Ô∏è 6. Model Development

### a) Model Selection

* What model(s) did you try?
* Why did you choose them?
* Baseline vs. more advanced options
* Are interpretability or speed factors?

### b) Dataset Construction

* How did you build your dataset?
  * Labeling (manual or natural?)
  * Sampling strategy?
  * Class balance?
* How did you split it? (train/val/test)

### c) Loss Function

* What loss did you use?
* Did you experiment with weighted or custom losses?

### d) Regularization

* L1, L2, dropout, etc.‚Äîwhat and why?

### e) Training Strategy

* Fine-tuned or trained from scratch?
* Optimization algorithm?
* Any tricks used (e.g., early stopping, batch size tuning)?


---

## üìè 7. Evaluation

### a) Offline Evaluation

* What metrics did you use during development?
  * Classification: Precision, Recall, F1, ROC-AUC
  * Regression: MAE, RMSE
  * Ranking: MRR, Precision@k, etc.
* Why these metrics?
* What are the results? Are they acceptable?

### b) Online Evaluation (if applicable)

* Are you simulating or actually running this live?
* What are the business-facing metrics? (CTR, revenue lift, etc.)
* What does a ‚Äúgood‚Äù outcome look like?


---

## üöÄ 8. Deployment and Serving

### a) Inference Environment

* Will this run in the cloud or on-device?
* Why?

### b) Model Compression (if needed)

* Did you use:
  * Knowledge distillation?
  * Pruning?
  * Quantization?

### c) Testing in Production

* How did you (or would you) test this model in production?
  * Shadow deployment?
  * A/B testing?
* What experiments would you run?

### d) Prediction Pipeline

* Online vs. batch prediction?
* What are the trade-offs?
* Is real-time feature access possible?


---

## üì° 9. Monitoring

### a) Failure Scenarios

* What could cause your model to fail in production?
* Will the data distribution change?
* What monitoring will catch that?

### b) What You Monitor

* **Operational Metrics**: Latency, throughput, resource usage
* **ML Metrics**: Drift, input/output validation, accuracy drop
* **Model Versions**: How do you know which version is running?


---

## üèóÔ∏è 10. System Architecture & MLOps Readiness

**Purpose:** Describe how the full system was architected, deployed, and operationalized.

**Prompts:**

* How are all components (simulation, streaming, ML, audit, dashboards) structured and deployed?
* What tools or platforms are involved? (Docker, Kafka, Cloud Run, MLflow, etc.)
* How is reproducibility ensured across environments?
* Are there MLOps or DevOps considerations? What‚Äôs the future CI/CD plan?

**Purpose:** Show awareness of real-world deployment requirements.

**Prompts:**

* How is the training pipeline structured?
* Are you using CI/CD or automated retraining?
* What tools or platforms are involved? (Airflow, Docker, SageMaker, etc.)
* Are there DevOps/MLOps challenges?


---

## ‚ö†Ô∏è 11. Risks and Limitations

**Purpose:** Show critical thinking beyond just implementation.

**Prompts:**

* What does your model struggle with?
* What are the biggest risks in deployment?
* Any ethical or bias concerns?
* What‚Äôs missing from your current solution?


---

## üß† 12. Reflection

**Purpose:** Capture your learning and identify improvements.

**Prompts:**

* What worked well in this project?
* What was harder than expected?
* If you started over, what would you change?
* What do you want to explore further?


---

## üß≠ 13. Strategic System Posture

**Purpose:** Explain how this system aligns with future business needs, regulatory pathways, and scaling objectives.

**Prompts:**

* How does this system prepare the company for future operational or regulatory requirements?
* How is this structured to scale across multiple SMR deployments?
* How does this interface with future digital twin implementations?
* What foundations does this lay for data, auditability, and reliability engineering?


---

## üìà 14. Proposal for Next Steps

**Purpose:** Demonstrate leadership vision and technical foresight.

**Prompts:**

* If this system were being adopted at the company, what would your next technical moves be?
* What technical debt needs to be paid down?
* What roles or hires would be needed to scale it?
* What new functionality would you add?
* What compliance or safety-related integrations would come next?


